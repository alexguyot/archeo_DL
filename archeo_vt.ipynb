{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detectron2 - Archeo dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare env."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure Detectron2 is installed before preparing env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check torch and gcc\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import standard lib.\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import logging\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "# import detectron2 and utilities\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine.hooks import HookBase\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.logger import log_every_n_seconds\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, DatasetMapper, build_detection_test_loader\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import build\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.evaluation import inference_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset home dir containing dataset\n",
    "ARCHEO_DIR = \"./datasets/example_dataset\"\n",
    "\n",
    "#Train and Validation folder in ARCHEO_DIR\n",
    "TRAIN_DIR = \"train\"\n",
    "VALIDATION_DIR = \"test\"\n",
    "\n",
    "#Output dir to store model\n",
    "OUTPUT_DIR = \"./outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotations must be in COCO format (eg. via_region_data.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_archeo_dicts(img_dir):\n",
    "    print (img_dir)\n",
    "    assert (os.path.isfile(os.path.join(img_dir, \"via_region_data.json\")))\n",
    "    json_file = os.path.join(img_dir, \"via_region_data.json\")\n",
    "    with open(json_file) as f:\n",
    "        imgs_anns = json.load(f)\n",
    "\n",
    "    dataset_dicts = []\n",
    "    for idx, v in enumerate(imgs_anns.values()):\n",
    "        record = {}\n",
    "        \n",
    "        filename = os.path.join(img_dir, v[\"filename\"])\n",
    "        height, width = cv2.imread(filename).shape[:2]\n",
    "        \n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "      \n",
    "        annos = v[\"regions\"]\n",
    "        objs = []\n",
    "        for _, anno in annos.items():\n",
    "            # assert not anno[\"region_attributes\"] #If this is not taken into account is monoclass\n",
    "            anno = anno[\"shape_attributes\"]\n",
    "            px = anno[\"all_points_x\"]\n",
    "            py = anno[\"all_points_y\"]\n",
    "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "\n",
    "            obj = {\n",
    "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": 0,\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n",
    "\n",
    "for d in [TRAIN_DIR, VALIDATION_DIR]:\n",
    "    DatasetCatalog.register(\"archeo_\" + d, lambda d=d: get_archeo_dicts(os.path.join(ARCHEO_DIR, d)))\n",
    "    MetadataCatalog.get(\"archeo_\" + d).set(thing_classes=[\"archeo\"])\n",
    "    print (MetadataCatalog.get(\"archeo_\" + d))\n",
    "\n",
    "train_dataset_meta = MetadataCatalog.get(\"archeo_\" + TRAIN_DIR)\n",
    "train_dataset = DatasetCatalog.get(\"archeo_\" + TRAIN_DIR)\n",
    "test_dataset_meta = MetadataCatalog.get(\"archeo_\" + VALIDATION_DIR)\n",
    "test_dataset = DatasetCatalog.get(\"archeo_\" + VALIDATION_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "\n",
    "# get config for pre-trained model (Mask R-CNN, Resnet-101)\n",
    "# https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n",
    "# get pre-trained weights\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")\n",
    "\n",
    "# Number of data loading threads\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "# To use cropping while data augmentation\n",
    "cfg.INPUT.CROP.ENABLED = True\n",
    "# Input image format\n",
    "cfg.INPUT.FORMAT = 'RGB'\n",
    "# Number of class\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "# Input image size\n",
    "cfg.INPUT.MIN_SIZE_TEST = 512\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = 512\n",
    "#Specify anchor sizes\n",
    "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[16, 32, 64, 128, 256, 512]]\n",
    "\n",
    "#Configuration of dataset and output\n",
    "cfg.DATASETS.TRAIN = (\"archeo_\" + TRAIN_DIR,)\n",
    "cfg.DATASETS.TEST = (\"archeo_\" + VALIDATION_DIR,)\n",
    "\n",
    "#Creating an output dir \n",
    "cfg.OUTPUT_DIR=OUTPUT_DIR\n",
    "\n",
    "#Configuration of solver\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "#Iterations in which the learning rate will be multiplied by gamma factor.\n",
    "cfg.SOLVER.STEPS = (500, 1000, 1500)\n",
    "#Learning rate - This learning rate will be used ONLY once the warmup period has ended\n",
    "cfg.SOLVER.BASE_LR = 0.002\n",
    "#Maximum iterations\n",
    "#The maximum of iterations is calculated by multiplying the amount of epochs times the amount of images times the images per batch\n",
    "cfg.SOLVER.MAX_ITER = 2000\n",
    "#Warmup iteration before adaptive LR\n",
    "cfg.SOLVER.WARMUP_ITERS = 100\n",
    "\n",
    "#Freeze all but heads\n",
    "cfg.FREEZE_AT = 4\n",
    "\n",
    "#Test performance on the DATASETS.TEST every EVAL_PERIOD\n",
    "cfg.TEST.EVAL_PERIOD = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(dataset_dict):\n",
    "    # Implement a mapper with data augmentation\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=cfg.INPUT.FORMAT)\n",
    "\n",
    "    image, transforms = T.apply_transform_gens([\n",
    "        T.RandomFlip(prob=0.50, horizontal=True, vertical=False),\n",
    "        T.RandomFlip(prob=0.50, horizontal=False, vertical=True),\n",
    "        T.RandomApply(T.RandomCrop(crop_type=\"relative_range\", crop_size=(0.8, 0.8)), \n",
    "                       prob=0.50),       \n",
    "        T.RandomApply(T.RandomRotation(angle=[-30,30], expand=True, center=None, sample_style=\"range\", interp=None), \n",
    "                      prob=0.50)\n",
    "    ], image)\n",
    "    \n",
    "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
    "        for obj in dataset_dict.pop(\"annotations\")\n",
    "        if obj.get(\"iscrowd\", 0) == 0\n",
    "    ]\n",
    "    instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
    "    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossEvalHook(HookBase):\n",
    "    def __init__(self, eval_period, model, data_loader):\n",
    "        self._model = model\n",
    "        self._period = eval_period\n",
    "        self._data_loader = data_loader\n",
    "    \n",
    "    def _do_loss_eval(self):\n",
    "        # Copying inference_on_dataset from evaluator.py\n",
    "        total = len(self._data_loader)\n",
    "        num_warmup = min(5, total - 1)\n",
    "            \n",
    "        start_time = time.perf_counter()\n",
    "        total_compute_time = 0\n",
    "        losses = []\n",
    "        for idx, inputs in enumerate(self._data_loader):            \n",
    "            if idx == num_warmup:\n",
    "                start_time = time.perf_counter()\n",
    "                total_compute_time = 0\n",
    "            start_compute_time = time.perf_counter()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            total_compute_time += time.perf_counter() - start_compute_time\n",
    "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
    "            seconds_per_img = total_compute_time / iters_after_start\n",
    "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
    "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
    "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
    "                        idx + 1, total, seconds_per_img, str(eta)\n",
    "                    ),\n",
    "                    n=5,\n",
    "                )\n",
    "            loss_batch = self._get_loss(inputs)\n",
    "            losses.append(loss_batch)\n",
    "        mean_loss = np.mean(losses)\n",
    "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
    "        comm.synchronize()\n",
    "\n",
    "        return losses\n",
    "            \n",
    "    def _get_loss(self, data):\n",
    "        # How loss is calculated on train_loop \n",
    "        metrics_dict = self._model(data)\n",
    "        metrics_dict = {\n",
    "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "            for k, v in metrics_dict.items()\n",
    "        }\n",
    "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
    "        return total_losses_reduced\n",
    "        \n",
    "        \n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        is_final = next_iter == self.trainer.max_iter\n",
    "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
    "            self._do_loss_eval()\n",
    "        self.trainer.storage.put_scalars(timetest=12)\n",
    "\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "                     \n",
    "    def build_hooks(self):\n",
    "        hooks = super().build_hooks()\n",
    "        hooks.insert(-1,LossEvalHook(\n",
    "            cfg.TEST.EVAL_PERIOD,\n",
    "            self.model,\n",
    "            build_detection_test_loader(\n",
    "                self.cfg,\n",
    "                self.cfg.DATASETS.TEST[0],\n",
    "                DatasetMapper(self.cfg,True)\n",
    "            )\n",
    "        ))\n",
    "        return hooks\n",
    "    \n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        return build.build_detection_test_loader(cfg, dataset_name, mapper=DatasetMapper(cfg, False))\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build.build_detection_train_loader(cfg, mapper=mapper) ## To apply transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load fine-tuned model\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "#Testing threshold for this model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "#Config predictor\n",
    "predictor = DefaultPredictor(cfg) \n",
    "#Dataset for evaluation\n",
    "#dataset = test_dataset_meta.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats. results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure evaluation\n",
    "evaluator = COCOEvaluator(test_dataset_meta.name, cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "val_loader = build_detection_test_loader(cfg, test_dataset_meta.name)\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual results (detected segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_plots = len(test_dataset)\n",
    "cols = 2\n",
    "\n",
    "rows = total_plots // cols \n",
    "rows += total_plots % cols\n",
    "pos = range(1,total_plots + 1)\n",
    "\n",
    "fig = plt.figure(1, figsize=(10*cols, 10*rows))\n",
    "\n",
    "for i, d in enumerate(test_dataset):\n",
    "    \n",
    "    #load image\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    \n",
    "    ax = fig.add_subplot(rows,cols,pos[i])\n",
    "    #load image\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "\n",
    "    #run prediction\n",
    "    outputs = predictor(im)\n",
    "\n",
    "    #get results prediction\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=test_dataset_meta, \n",
    "                   scale=1\n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    ax.set_title('Prediction: ' + d[\"file_name\"].split('/')[-1])\n",
    "    ax.imshow(out.get_image())\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
